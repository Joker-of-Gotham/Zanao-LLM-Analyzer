# zanao_analyzer\execution\run_batch_analytics.py

# èŒè´£ï¼šä½œä¸ºä¸€ä¸ªå¯ä»¥å®šæœŸæ‰§è¡Œï¼ˆä¾‹å¦‚ï¼Œæ¯å¤©å‡Œæ™¨æ‰§è¡Œä¸€æ¬¡ï¼‰çš„è„šæœ¬ï¼Œè´Ÿè´£è¿›è¡Œæ¶ˆè€—èµ„æºçš„æ·±åº¦åˆ†æå’Œå…¨å±€ç»Ÿè®¡ã€‚
# æµç¨‹ï¼š
# è¿æ¥åˆ†æç»“æœæ•°æ®åº“ analysis.dbã€‚
# è°ƒç”¨ statistics_engine ä¸­çš„å„ç§æ–¹æ³•ï¼Œè¿›è¡Œå…¨å±€è®¡ç®—ï¼ˆTop-Kç”¨æˆ·ã€æ–°è¯å‘ç°ã€çƒ­å¸–è¶‹åŠ¿ç­‰ï¼‰ã€‚
# å°†è®¡ç®—ç»“æœæ›´æ–°åˆ° analysis.db ä¸­å¯¹åº”çš„ç»Ÿè®¡è¡¨é‡Œã€‚
# è°ƒç”¨ similarity_engineï¼Œå¯¹æ–°åˆ†æçš„å¸–å­è¿›è¡Œåˆ†ç±»åŒ¹é…ï¼Œä¿å­˜ç»“æœã€‚

# -*- coding: utf-8 -*-
"""
ã€æ ¸å¿ƒã€‘æ‰¹é‡åˆ†æä¸ç»Ÿè®¡è„šæœ¬ - ã€V4ï¼Œä½¿ç”¨æ–°çš„post_classificationsè¡¨ã€‘
- ä½œä¸ºä¸€ä¸ªå¯ä»¥å®šæœŸæ‰§è¡Œçš„è„šæœ¬ï¼Œè´Ÿè´£è¿›è¡Œæ¶ˆè€—èµ„æºçš„æ·±åº¦åˆ†æå’Œå…¨å±€ç»Ÿè®¡ã€‚
"""
import sys
import os
import sqlite3
import json
import numpy as np

# --- åŠ¨æ€è·¯å¾„ä¿®å¤ï¼Œç¡®ä¿å¯ä»¥ä»ä»»ä½•ä½ç½®è¿è¡Œ ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)
# --- è·¯å¾„ä¿®å¤ç»“æŸ ---

import config
from core.statistics_engine import StatisticsEngine
from core.similarity_engine import SimilarityEngine

def run_statistics_module(conn: sqlite3.Connection):
    """è¿è¡Œæ‰€æœ‰ç»Ÿè®¡è®¡ç®—æ¨¡å—"""
    print("\n--- Running Statistics Module (Full) ---")
    # å°† config å¯¹è±¡ä¼ é€’ç»™ StatisticsEngine
    stats_engine = StatisticsEngine(conn, config)
    
    # ä¾æ¬¡è°ƒç”¨æ‰€æœ‰é«˜çº§ç»Ÿè®¡æ–¹æ³•
    stats_engine.calculate_entity_frequencies()
    stats_engine.analyze_user_relations(top_k=20)
    stats_engine.track_hot_post_trends(time_window_days=7, top_k=10)
    stats_engine.detect_new_words(recent_days=7, historical_days=30, top_k=20)
    
    print("--- Statistics Module Finished ---")

def run_classification_module(conn: sqlite3.Connection):
    """
    è¿è¡Œå¸–å­åˆ†ç±»æ¨¡å— (åŸsimilarity_module)ï¼Œå°†ç»“æœå­˜å…¥æ–°çš„ post_classifications è¡¨ã€‚
    """
    print("\n--- Running Post Classification Module ---")
    sim_engine = SimilarityEngine() # SimilarityEngine æˆåŠŸåˆå§‹åŒ–
    cursor = conn.cursor()
    
    try:
        # ... (æŸ¥æ‰¾æœªåˆ†ç±»å¸–å­çš„ SQL æŸ¥è¯¢ï¼Œæ— éœ€ä¿®æ”¹) ...
        cursor.execute("""
            SELECT ba.id, ba.entities_json
            FROM base_analysis ba
            LEFT JOIN post_classifications pc ON ba.id = pc.base_analysis_id
            WHERE pc.id IS NULL AND ba.entities_json IS NOT NULL;
        """)
        posts_to_classify = cursor.fetchall()
        
        if not posts_to_classify:
            print("No new posts found for classification. Skipping.")
            return

        print(f"Found {len(posts_to_classify)} new posts to classify...")
        
        data_to_insert = []
        for post_id, entities_json in posts_to_classify:
            try:
                entities = json.loads(entities_json)
                entity_texts = [e['text'] for e in entities if e.get('text')]
            except (json.JSONDecodeError, TypeError):
                continue
            
            if not entity_texts: continue

            # âœ…âœ…âœ… æ ¸å¿ƒä¿®æ­£ âœ…âœ…âœ…
            # 1. å°†å®ä½“åˆ—è¡¨æ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²
            query_text_from_entities = " ".join(entity_texts)
            
            # 2. è°ƒç”¨æ­£ç¡®çš„ã€å­˜åœ¨çš„æ–¹æ³•åï¼Œå¹¶å°†æ‹¼æ¥åçš„å­—ç¬¦ä¸²ä¼ å…¥
            #    æˆ‘ä»¬åªå–æœ€åŒ¹é…çš„é‚£ä¸€ä¸ªåˆ†ç±» (top_k=1)
            matches = sim_engine.match_query_to_classification(query_text_from_entities, top_k=1)
            # ğŸ”´ åˆ é™¤é”™è¯¯çš„ä¸€è¡Œ: matches = sim_engine.match_entities_to_classification(entity_texts)
            
            for match in matches:
                # å‡†å¤‡æ’å…¥æ–°è¡¨çš„æ•°æ®å…ƒç»„
                # è¿™é‡Œæœ‰ä¸€ä¸ªå°é—®é¢˜ï¼šæˆ‘ä»¬ä¸çŸ¥é“æ˜¯å“ªä¸ªæºå®ä½“åŒ¹é…ä¸Šäº†
                # ä¸€ä¸ªç®€å•çš„å¤„ç†æ˜¯ï¼Œæˆ‘ä»¬è®°å½•æ‹¼æ¥åçš„æ–‡æœ¬
                # æˆ–è€…ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥éå†æ‰€æœ‰å®ä½“ï¼Œä½†è¿™æ ·æ•ˆç‡è¾ƒä½
                # å…ˆç”¨ä¸€ä¸ªç®€å•çš„æ–¹å¼ï¼Œè®°å½•ç¬¬ä¸€ä¸ªå®ä½“ä½œä¸ºæºå®ä½“
                source_entity = entity_texts[0] if entity_texts else 'unknown'
                
                data_to_insert.append((
                    post_id,
                    source_entity,              # æºå®ä½“
                    match['classification'],    # åŒ¹é…åˆ°çš„åˆ†ç±»
                    match['score']              # åˆ†æ•°
                ))
        
        # æ‰¹é‡æ’å…¥æ‰€æœ‰æ‰¾åˆ°çš„åŒ¹é…ç»“æœ
        if data_to_insert:
            cursor.executemany(
                "INSERT INTO post_classifications (base_analysis_id, source_entity_text, matched_classification, match_score) VALUES (?, ?, ?, ?);",
                data_to_insert
            )
            conn.commit()
            print(f"SUCCESS: Inserted {len(data_to_insert)} new classification matches into 'post_classifications'.")
            
    except Exception as e:
        conn.rollback()
        print(f"[ERROR] Failed during classification module: {e}")

    print("--- Post Classification Module Finished ---")


def main():
    """ä¸»å‡½æ•°ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ‰¹å¤„ç†ä»»åŠ¡"""
    print("====== Batch Analytics Script (Schema v2) Started ======")
    conn = None
    try:
        # ä½¿ç”¨ with è¯­å¥ç¡®ä¿æ•°æ®åº“è¿æ¥æ€»èƒ½è¢«å…³é—­
        with sqlite3.connect(config.ANALYSIS_DB_PATH) as conn:
            run_statistics_module(conn)
            run_classification_module(conn)
    except Exception as e:
        print(f"[FATAL] An unexpected error occurred: {e}")
    
    print("\n====== Batch Analytics Script Finished ======")

if __name__ == '__main__':
    main()